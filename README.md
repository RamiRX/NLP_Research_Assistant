# NLP_Research_Assistant
Official Repository of the Unsupervised Project

*A balanced scientific dataset for NLP vs. Non-NLP document classification*

This repository contains the complete pipeline used to build a high-quality, balanced dataset for the **Doculizer** research assistant.  
The goal of this stage is to automatically gather, clean, filter, and structure scientific papers in order to train a robust classifier that detects whether an uploaded document is **NLP-related** or **Non-NLP**.

---



## ðŸ—‚ Dataset Composition

| Category | Count | Description |
|----------|-------|-------------|
| **NLP-related** | 1200 | Scientific papers from arXiv (cs.CL) + Semantic Scholar queries |
| **Non-NLP** | 1200 | Scientific papers from non-NLP domains (Physics, Math, Biology, CV, Roboticsâ€¦) |

Total documents: **2400**

Each document includes:
- `paper_id`
- `title`
- `abstract`
- `label` (nlp_related / not_nlp)
- `subcategory` (for NLP papers)
- `source`
- `filepath` (local .txt file containing the cleaned text)

---

## ðŸ“š Data Sources

The dataset is collected from two major academic repositories:

### **1. arXiv**
Used for both:
- NLP papers: `cs.CL`
- Non-NLP papers:  
  `physics`, `math.AG`, `math.AP`, `q-bio`, `cs.CV`, `cs.LG`, `cs.RO`, `stat.ML`

### **2. Semantic Scholar**
Used to enrich the NLP category with keyword-based queries:
- "NLP"
- "natural language processing"
- "machine translation"
- "summarization"
- "question answering"

> Note: API-based collection is restricted; we fallback to arXiv when rate limits occur.

---

## ðŸ§¹ Preprocessing & Cleaning Pipeline

Each paper undergoes several preprocessing steps:

1. **Whitespace normalization**
2. **Control character cleaning**
3. **Abstract length filtering (â‰¥ 50 tokens)**
4. **Keyword-based exclusion to avoid NLP contamination in Non-NLP papers**
5. **Subcategory assignment for NLP papers**  
   (e.g., MT, QA, Summarization, NER, Sentiment, Dialogue, LLM)

---

## ðŸ”Ž Duplicate Removal (Fuzzy Matching)

To guarantee dataset quality, a fuzzy deduplication step is applied using:

- **SHA1 fingerprinting** (exact duplicates)
- **FuzzyWuzzy â€“ token_sort_ratio â‰¥ 90** (semantic duplicates)

This ensures:
- No repeated papers  
- No near-duplicates  
- No noisy titles/abstracts

---

## âš–ï¸ Dataset Balancing

After collection and deduplication:

- The NLP and Non-NLP sets are randomly shuffled  
- Each category is trimmed to the target size:  
  **1200 + 1200 documents**

This avoids class imbalance and prevents overfitting during classifier training.

---

---

# ðŸ“˜ **SciBERT Classification Module**

This module implements the **core binary classifier** used in *Doculizer* to determine whether an academic paper is **NLP-related** or **not NLP-related**.
The classifier acts as the **first logical gate** of the system: only papers classified as NLP-related proceed to the downstream pipeline.

---

## ðŸ”¬ **Model Choice: Why SciBERT?**

We selected **SciBERT (allenai/scibert_scivocab_uncased)** for fine-tuning because:

* It is **pretrained on 1.14M scientific papers**, making it inherently aligned with our domain.
* It performs significantly better than general-domain BERT models on scientific tasks.
* It requires **minimal compute** to achieve high accuracy on short-text classification (title + abstract).

SciBERT offers the best balance between:

* **Performance**
* **Training efficiency**
* **Low carbon footprint**
* **Domain adaptation**

---

## âš™ï¸ **Training Setup**

| Component                   | Configuration                           |
| --------------------------- | --------------------------------------- |
| Base model                  | SciBERT (uncased scientific vocabulary) |
| Task                        | Binary classification (NLP vs. Non-NLP) |
| Input                       | Title + Abstract concatenated           |
| Max sequence lengths tested | 16, 32, 64                              |
| Optimizer                   | AdamW                                   |
| Loss function               | Cross-entropy                           |
| Batch size                  | 8                                       |
| Epochs tested               | 3, 5, 10                                |
| Validation split            | 15%                                     |
| Test split                  | 20%                                     |

We trained multiple configurations to find the optimal trade-off between accuracy and energy consumption.

---

## ðŸ“Š **Evaluation Metrics**

We report:

* **Precision**
* **Recall**
* **F1-score**
* **Accuracy**

And additionally, we measured **COâ‚‚ emissions** using the `codecarbon` library to ensure environmental transparency.

---

## ðŸ§ª **Ablation Study Results**

### **Final Comparison of Experiments**

| Experiment             | Epochs | Max Tokens | Train F1 | Val F1 | Test F1  | Carbon (g COâ‚‚eq) |
| ---------------------- | ------ | ---------- | -------- | ------ | -------- | ---------------- |
| **Best (recommended)** | **3**  | **64**     | 1.00     | 0.99   | **0.99** | **1.965 g**      |
| Exp 2                  | 5      | 32         | 1.00     | 0.99   | 0.99     | 2.199 g          |
| Exp 3                  | 5      | 16         | 1.00     | 0.98   | 0.97     | 4.958 g          |
| Exp 4                  | 10     | 32         | 1.00     | 0.99   | 0.98     | 10.730 g         |

---

## ðŸ† **Chosen Model**

We selected the configuration:

### **SciBERT + 3 Epochs + 64 Max Tokens**

Because it provides:

* **State-of-the-art performance** on our dataset
* **High generalization** without overfitting
* **Best environmental footprint**
* **Fast inference suitable for production**

This model achieved:

```
Test Precision: 0.99
Test Recall:    0.99
Test F1-score:  0.99
```
Model URL on Google Drive:
https://drive.google.com/file/d/1-uf26t8kUTh60O16q8eVGdaeD9oOHlzY/view?usp=sharing
---

## ðŸŒ± **Carbon-Aware Machine Learning**

Training experiments were monitored using **CodeCarbon**, ensuring:

* Transparent reporting of energy consumption
* Reduced environmental impact
* Responsible ML practices aligned with modern research standards

---

## ðŸ”Ž Transition to Semantic Retrieval

While **SciBERT** is highly effective for the initial binary classification (filtering NLP vs. non-NLP papers), it is not optimized for ranking documents based on query relevance.

The next stage of the project introduces **Sentence-BERT (SBERT)** models to handle semantic retrieval and ranking, replacing traditional TF-IDF-based approaches.

### Semantic Retrieval with SBERT

#### Objective

After filtering papers with SciBERT, we rank the accepted papers by semantic relevance to a user's query. Unlike classical bag-of-words approaches (e.g., TF-IDF), our objective is to capture **semantic similarity** beyond exact lexical overlap. This is crucial in scientific literature where similar concepts are often described using different terminology.

#### Models Compared

We evaluated three Sentence-BERT (SBERT) retrievers:

* **MiniLM** (`all-MiniLM-L6-v2`): A lightweight and efficient model, commonly used as a baseline for semantic search.
* **MPNet** (`all-mpnet-base-v2`): A higher-capacity model designed to produce more expressive sentence embeddings, often yielding stronger ranking performance.
* **DistilRoBERTa**: A compressed transformer model that trades some accuracy for reduced computational cost.

All models utilize the same retrieval paradigm (dense embeddings + cosine similarity) but differ in architecture and pretraining objectives.

### Evaluation Protocol

To evaluate semantic retrieval in a controlled and reproducible manner, we use the **SciFact** dataset.

**Why SciFact?**

* **Scientific Domain:** Composed of real scientific abstracts, aligning perfectly with our academic paper retrieval use case.
* **Ground-Truth Relevance:** Each query (claim) is paired with verified supporting/refuting documents, enabling objective automatic evaluation.
* **Closed-Corpus Setting:** Matches our system design where retrieval is performed over a specific set of user-provided papers.

#### Metrics

We report standard ranking-based retrieval metrics at cutoff :

* **nDCG@10:** Evaluates ranking quality by rewarding relevant documents appearing at higher ranks.
* **MAP@10:** Measures average precision across all queries, capturing ranking consistency.
* **Recall@10:** Indicates whether relevant documents are retrieved within the top 10 results.
* **Precision@10 (P@10):** Measures the proportion of relevant documents in the top 10. *Note: This is typically low in scientific retrieval as usually only 1-2 documents are relevant per query.*

### ðŸ“Š Results

| Model | nDCG@10 | MAP@10 | Recall@10 | P@10 |
| --- | --- | --- | --- | --- |
| **MPNet** | **0.65570** | **0.60827** | **0.79011** | **0.08933** |
| MiniLM | 0.64508 | 0.59593 | 0.78333 | 0.08833 |
| DistilRoBERTa | 0.63146 | 0.57580 | 0.78789 | 0.08900 |

> **Note on Metrics:** It is crucial to distinguish between classification and retrieval metrics. While classification F1-scores often exceed 0.95, retrieval tasks involve ranking a large set of candidates for ambiguous queries. Consequently, nDCG and MAP values in the range of **0.60â€“0.70** represent strong performance in scientific retrieval.

### âœ… Model Selection

Based on the evaluation results, we selected **MPNet** (`all-mpnet-base-v2`) as the default semantic retriever. It achieves the highest nDCG@10 and MAP@10, indicating superior ranking quality and robustness. MiniLM remains available as a fallback for resource-constrained environments.


